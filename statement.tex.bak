SMITLe works in the following scenario. There is a image dataset (source data) containing $N$ categories and a set of binary classifiers are trained from this dataset to distinguish these $N$ categories. However, we can only access to the classifiers rather than the data itself. Now we collect our own image dataset (target task) coming from $N+1$ categories. This target dataset consists of $N$ identical categories to the source data and one new category related to the previous $N$ categories. In order to solve our new task, we would expect our classifier to get improved performance with respect to 
\begin{itemize}
\item Exploiting knowledge from related source models. If these two tasks are related, SMITLe should transfer the prior knowledge aggressively. In some cases where the prior knowledge is very related and training size of the target task is small, the final decision of the classifier could be mainly rely on the decision from the source models.
\item Disposing unrelated knowledge. If the knowledge between these two tasks is unrelated, the algorithm should be able to distinguish and dispose the unrelated knowledge autonomously. In the worst case, none of the prior knowledge is related and SMITLe should show similar performance as the model trained merely from target data.
\end{itemize}

In this paper, we use LS-SVM as the classifier for the multi-class incremental transfer problem. In the following, we briefly introduce the mathematical setting of our problem.

\subsection{Mathematical Setting and Definition}
Here we introduce the mathematical setting and notations used in the rest of the paper. In general, we use any letter with apostrophe to denote the information from the source data, e.g. if $f(x)$ denotes the model for the target task, $f'(x)$ denotes the model for the source one. We show all the notations in Table \ref{tab:notation}.
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{useful notations in this paper}
    \begin{tabular}{cL{7cm}}
    \hline
    $f',f$ & binary model for source and task task respectively\\
    \hline
    $\phi(x)$ &  function mapping the input sample into a high dimensional feature space. \\ \hline
%    $K(x,x)$ & kernel matrix with  $\phi(x_i) \cdot\phi(x_j)$ corresponding to its element $(i,j)$\\ \hline
    $X$     & instance matrix with each row representing one instance \\\hline
    $\boldsymbol{W} $    & (N+1)-column hyperplane matrix for target task; each column represents one hyperplane of a binary model \\\hline
    $\boldsymbol{W'}$    & N-column hyperplane matrix for the source task \\\hline
    $\boldsymbol{a'} $   & the Lagrangian multiplier matrix for source problem. Each column represents the the Lagrangian multipliers for a binary model  \\\hline
    $\boldsymbol{a} $    & the Lagrangian multiplier matrix for target problem \\
    \hline
    $\boldsymbol{b'},\boldsymbol{b}$  & the bias vector for source and target task \\
        \hline
    $\boldsymbol{a_i}$ & $i_{th}$ column of matrix $\boldsymbol{a}$ \\ \hline
%    $d_\gamma$ &  diagonal matrix with$\left[ {{\gamma _1},...,{\gamma _N}} \right]$ in its main diagonal\\\hline
    $\boldsymbol{\beta}$ & row vector $\left[ {{\beta _1},...,{\beta _N}} \right]$ to control the prior knowledge for the new category\\ \hline
    $\varepsilon_{ny_i}$&loss parameter. $\varepsilon _{n{y_i}}=1$ if $n=y_i$ and 0 otherwise\\ \hline
    $\psi$, $\psi^{-1}$ & $\psi$ is the modified kernel matrix for solving binary LS-SVM and $\psi^{-1}$ is the inverse matrix of $\psi$\\ \hline
    \end{tabular}%
  \label{tab:notation}%
\end{table}%

 

Assume that, for our $(N+1)$-category target task, ${x} \in \mathcal{X}$ and ${y} \in \mathcal{Y}=\left\{1,2,...,N+1\right\}$ are the input vector and output for the learning task respectively. Meanwhile, we have a set of binary linear classifiers $f'_n(x)=\phi(x)w_n'+b_n'$, for $n=1,...,N$ trained from an unknown distribution with One-Versus-All (OVA) strategy.  Now we want to learn a set of classifiers $f_n(x)=\phi(x)w_n+b_n, n=1,...,N+1$ for our new task, so that example $x$ is assigned to the category $j$ if $j \equiv \arg {\max _{n = 1,...,N+1}}\left\{{f_n}(x)\right\}$. In LS-SVM, the solution of the model parameters $(w_n,b_n)$ can be found by solving the following optimization problem:
\begin{equation*}
\begin{aligned}
\textbf{min} && R({w_n}) + \frac{C}{2}\sum\limits_i^l {({Y_{i,n}} - \phi ({x_i}){w_n} - {b_n})^2} \\
\end{aligned}
\end{equation*}
Where $R({w_n})$ is the regularization term to guarantee good generalization performance and avoid overfitting. $\mathbf{Y}$ is a encoded label matrix so that $Y_{in}=1$ if $y_i=n$ and $-1$ otherwise.  

In classic LS-SVM setting, the regularization term is set to $\frac{1}{2}\left\|w_n\right\|^2$ and the optimal $w_n=\phi(X)^T\alpha_n$ while the parameters $(\alpha_n,b_n)$ can be found by solving
\begin{equation}\label{eq:linear}
\left[ {\begin{array}{*{20}{c}}
{K(X,X) + \frac{1}{C}{\rm{I}}}&\mathbf{1}\\
\mathbf{1^T}&0
\end{array}} \right]\left( \begin{array}{l}
{\alpha _n}\\
{b_n}
\end{array} \right) = \left( \begin{array}{l}
{Y_n}\\
0
\end{array} \right)  
\end{equation}
Here $I$ is the identity matrix and $\mathbf{1}$ is a column vector with all its elements equal to 1.

Now our task can be divided into two separate part: learning the $N$ overlapped categories and the new category. 
As we know that the source and target share $N$ categories, from previous work \cite{yang2007cross}, the regularization term can be written as $\frac{1}{2}{{{\left\| {{w_n} - {\gamma _n}{{w'}_n}} \right\|}^2}}$. Here, $\gamma_n$ is the regularization parameter controlling the amount of transfer. $\gamma_n$ is expected to be big if the data distribution of the source and target are similar. Otherwise, $\gamma_n$ should be small or even negative to prevent negative transfer.  
For learning the new category, we use multi-source kernel learning strategy from \cite{tommasi2014learning}, leveraging knowledge from multiple sources.  

Therefore, our multi-class incremental transfer problem can be solved by optimizing the following objective function:
\begin{equation}
\begin{aligned}
\textbf{min}\qquad {} & \frac{1}{2}\sum\limits_{n = 1}^N {{{\left\| {{w_n} - {\gamma _n}{{w'}_n}} \right\|}^2}}  + \frac{1}{2}{\left\| {{w_{N + 1}} - \sum\limits_{k = 1}^N {w{'_k}{\beta _k}} } \right\|^2}\\& \frac{C}{2}\sum\limits_{n = 1}^{N + 1} {\sum\limits_{i = 1}^l {e_{i,n}^2} }  \\
\textbf{s.t.}\qquad {} &{e_{i,n}} = {Y_{in}} - \phi ({x_i}){w_n} - {b_n}
\end{aligned}\label{eq:opt}
\end{equation}

The the optimal solution to  Eq. (\ref{eq:opt}) is:
\begin{equation*}
\begin{array}{*{20}{c}}
{{w_n} = {\gamma _n}{{w'}_n} + \sum\limits_i^l {{\alpha _{in}}{\phi(x_i)}} }&{n = 1,...,N}\\
{{w_{N + 1}} = \sum\limits_k^N {{\beta _k}{{w'}_k}}  + \sum\limits_i^l {{\alpha _{i(N + 1)}}{\phi(x_i)}} }&{}
\end{array}
\end{equation*}
Here $\alpha_{ij}$ is the element $(i,j)$ in $\boldsymbol{\alpha}$. The intuitive interpretation of the result above is that the hyperplane of the target problem is the linear combination of the prior knowledge (first part of the right side) and empirical knowledge from target task (second part of the right side).

Let $\psi$ denotes the first term of left-hand side in Eq. (\ref{eq:linear}) and let:
\begin{equation}
\begin{array}{c}
 {\psi}\left[ {\begin{array}{*{20}{c}}
{\boldsymbol{\alpha} '}\\
{\boldsymbol{b}'}
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
Y\\
0
\end{array}} \right]\\
{\psi}\left[ {\begin{array}{*{20}{c}}
{\boldsymbol{\alpha} ''}\\
{\boldsymbol{b}''}
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
{X{{\left( {W'} \right)}^T}}\\
0
\end{array}} \right]
\end{array}
\end{equation}
We have:
\begin{equation}\label{eq:solution}
 \boldsymbol{\alpha}  = \boldsymbol{\alpha} ' - \left[ {\begin{array}{*{20}{c}}
 {\boldsymbol{\alpha} ''\boldsymbol{d_{\gamma}}}&{{\boldsymbol{\alpha} ''\boldsymbol{\beta ^T}}}
 \end{array}} \right]
\end{equation}
Here $\boldsymbol{d_{\gamma}}$ is a diagonal matrix with $\{\gamma_i\}_{i=1,...,N}$ in its main diagonal. From Eq. (\ref{eq:solution}) we can see that, the solution of Eq. (\ref{eq:opt}) is completed once $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ are set.

