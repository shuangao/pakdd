In this section, we show empirical results of our algorithm for different transferring situations on two image benchmark datasets: Office and Caltech.
\subsection{Dataset \& Baseline methods}
Office contains 31 classes from 3 subsets (Amazon,Dslr and Webcam) and Caltech contains 256 classes. We select 13 shared classes from two datasets\footnote{13 classes include: backpack, bike, helmet, bottle, calculator, headphone, keyboard, laptop, monitor, mouse, mug, phone and projector}. The input features of all examples are extracted using AlextNet\cite{krizhevsky2012imagenet}.
%Because the two subsets Dslr and Webcam are relatively small and don't have data for testing, we only use them as the source domain.
\begin{table}[htbp]
	\centering
	\caption{Statistics of the datasets and subsets}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Dataset&Subsets&\# classes &\# examples & \# features\\\hline
		\multirow{3}{*}{Office} & Amazon &13&1173 & 4096\\
		
		& Dslr &13&224 & 4096\\
		& Webcam &13&369 & 4096\\
		\hline
		Caltech256&Caltech&13&1582&4096\\
		\hline
	\end{tabular}%
	\label{tab:class_info}%
\end{table}%
We compare our algorithm EMTLe with two kinds of baselines. The first one is the methods without leveraging any source knowledge (no transfer baselines), including two methods. \textbf{No transfer:} SVMs trained only on target data. Any transfer algorithm that performs worse than it suffers from negative transfer. \textbf{Batch:} We combine the source and target data, assuming that we have full access to all data, to train the SVMs. The result of the Batch method is expected to outperform other methods under the HTL setting as it can access the source data. The second kind of baseline consists of two previous transfer methods in HTL, \textbf{MKTL\cite{jie2011multiclass}} and \textbf{Multi-KT\cite{tommasi2014learning}}. Similar to EMTLe, both of them use the LOOCV method to estimate the relatedness of the source model and target domain, but they use their own convex objective function without the $\ell_2$ penalty terms. We use linear kernel for all methods in all our experiments.
\subsection{Transfer from Single Source Domain}
In this subsection, following the protocol in \cite{jie2011multiclass,tommasi2014learning} for fair comparison, we perform 12 groups of experiments under the setting of HTL. 
For each experiment, one of the 4 (sub)datasets is selected as the source, while another dataset is used as the target. We evaluate the the effectiveness of EMTLe when all source models are of the same type (linear SVMs).
The size of each target dataset is varied from 1 to 5 to see how EMTLe and other baselines behave under the extremely small dataset.
We perform each experiment 10 times and report the average result in Figure \ref{fig:exp}. 
\input{figs.tex}

\subsection{Transfer from Multiple Source Domains}
As we mentioned, EMTLe can exploit knowledge from different types of source classifiers which could greatly extend our selection of the source domain under the HTL setting. In this subsection we show that EMTLe can successfully transfer the knowledge from two source models of different types of source classifiers. Meanwhile, MKTL is used as our baseline which is also compatible with different types of source classifiers. 

In this experiment, we assume that there is no single source domain that can cover all classes in our target domain and we have to select source models from different source domains. Specifically, the 13 binary source models are selected from two different domains separately (6 from DSLR and 7 from Webcam) according to Table \ref{tab:class_gen}. Similar to our previous experiment configurations, we only use Caltech and Amazon as the target domains. We show the experiment results in Figure \ref{fig:exp2}.
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
	\centering
	\caption{The selected classes of the two source domains and the classifier type of the source model.}
	\begin{tabular}{|c|c|c|}
		\hline
		& class & classifier\\
		\hline
		DSRL& monitor,bike, helmet,calcu,headphone,projector & Logitic\\\hline
		Webcam&keyboard,mouse,phone,backpack,mug,bottle,laptop&SVMs\\ \hline
		
	\end{tabular}%
	\label{tab:class_gen}%
\end{table}%
\begin{figure}[th]
	\centering
	\subfigure[Dslr+Webcam $\rightarrow$ Amazon]{
		\includegraphics[width=0.4\textwidth]{fig/multi_amazon.png}\label{a2}
	}\qquad\qquad
	\subfigure[Dslr+Webcam $\rightarrow$ Caltech]{
		\includegraphics[width=0.4\textwidth]{fig/multi_caltech.png}\label{b2}
	}\\
	\caption{Recognition Accuracy for Multi-Model \& Multi-Source experiment on two target datasets. }
	\label{fig:exp2}
\end{figure}

\textbf{Observation \& discussion:} Under our multi-source scenario, it is more difficult to leverage the knowledge from the source models as the models are trained from different domains. From the results we can see that, in this complex situation, EMTLe can still transfer the knowledge from the source models despite the type of the source classifiers while MKTL can hardly leverage the source knowledge. EMTLe uses a simple way to leverage the source models and BO can help us better estimate the transfer parameter. However, MKTL uses a sophisticated feature augmentation to leverage the source models and has more hyperparameters to estimate. With a few training data, it is difficult for MKTL to measure the importance of each source model and exploit the knowledge from the models effectively.




