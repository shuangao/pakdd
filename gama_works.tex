The motivation of transferring knowledge between different domains is to apply the previous information from the source domain to the target one, assuming that there exists a certain relationship, explicit or implicit, between the feature space of these two domains \cite{pan2010survey}. Technically, previous work can be categorized into solving the following three issues: \textit{what}, \textit{how} and \textit{when} to transfer \cite{tommasi2014learning}.


\textbf{What to transfer.} Previous work tried to answer this question from three different aspects: (1) selecting transferable instances, (2) learning transferable feature representations and (3) transferable model parameters. Instance-based transfer learning assumes that part of the instances in the source domain could be re-used to benefit the learning for the target domain. Lim et al. \cite{lim2012transfer} proposed a method of augmenting the training data by borrowing data from other classes for object detection. Learning transferable features means to learn common features that can alleviate the bias of data distribution in the target domain. Recently, Long et al. \cite{LongICML15} proposed a method that can learn transferable features using deep neural network and showed some impressive results on the  benchmarks. A model transfer
approach assumes that the parameters of the model for the source task can be transferred to the target task. Yang et al. \cite{yang2007cross} proposed Adaptive SVMs transferring parameters by incorporating the auxiliary classifier trained from the source domain. In addition to Yang's work, Ayatar et al. \cite{aytar2011tabula} proposed PMT-SVM that can determine the transfer regularizer automatically according to the target data. Tommasi et al. \cite{tommasi2014learning} proposed Multi-KT that can utilize the parameters from multiple source models for the target classes .
Kuzborskij et al. \cite{kuzborskij2013n} proposed a similar method to learn new categories by leveraging the known source models.

\textbf{When and how to transfer.} The question \textit{when to transfer} arises when we want to know if the information acquired from the previous task is relevant to the new one (i.e. in what situations knowledge should not be transferred). 
\textit{How to transfer} the prior knowledge effectively should be carefully designed to prevent inefficient and negative transfer. Previous work \cite{davis2009deep} \cite{wang2014active} \cite{zhou2014multi} has used the generative probabilistic method. Bayesian learning methods can predict the target domain by combining the prior source distribution to generate a posterior distribution. Alternatively, max margin methods \cite{kuzborskij2013n} \cite{tommasi2010safety} show that it is possible to learn from a few examples by minimizing the  Leave-One-Out (LOO) error for the training model. Cawley et al. \cite{cawley2006leave} show that there is a closed-form implementation of LOO cross-validation that can generate an unbiased model estimation for LS-SVM.

Our work corresponds to the context above. In this paper, we propose SMTLe based on the model transfer approach with LS-SVM. Our work addresses how to prevent negative transfer while only the source model is accessible for domain adaptation. Compared to other \hl{works}, we propose a new perspective which provides insight on negative transfer. 
Based on this, we propose our novel objective function and show that SMTLe can better leverage knowledge from different source models. As a result, SMTLe can achieve a better performance and alleviate negative transfer.
