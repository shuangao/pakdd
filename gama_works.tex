The motivation of transferring knowledge between different domains is to apply the previous information from the source domain to the target one, assuming that there exists a certain relationship, explicit or implicit, between the feature space of these two domains \cite{pan2010survey}. Technically, previous work can be categorized into solving the following three issues: \textit{what}, \textit{how} and \textit{when} to transfer \cite{tommasi2014learning}.


\textbf{What to transfer.} Previous work tried to answer this question from three different aspects: (1) selecting transferable instances, (2) learning transferable feature representations and (3) transferable model parameters. Instance-based transfer learning assumes that part of the instances in the source domain could be re-used to benefit the learning for the target domain. Lim et al.\cite{lim2012transfer} proposed a method of augmenting the training data by borrowing data from other classes for object detection. Learning transferable features means to learn common features that can alleviate the bias of data distribution in the target domain. Recently, Long et al. \cite{LongICML15} proposed a method that can learn transferable features using deep neural network and showed some impressive results on the  benchmarks. 
A model transfer approach assumes that the parameters of the model for the source task can be transferred to the target task. Yang et al. \cite{yang2007cross} proposed Adaptive SVMs transferring parameters by incorporating the auxiliary classifier trained from the source domain. 
In addition to Yang's work, Ayatar et al. \cite{aytar2011tabula} proposed PMT-SVM that can determine the transfer regularizer automatically according to the target data. 
Tommasi et al. \cite{tommasi2014learning} proposed Multi-KT that can utilize the parameters from multiple source models for the target classes .
Kuzborskij et al. \cite{kuzborskij2013n} proposed a similar method to learn new categories by leveraging the known source models.

\textbf{When and how to transfer.} The question \textit{when to transfer} arises when we want to know if the information acquired from the previous task is relevant to the new one (i.e. in what situations knowledge should not be transferred). 
\textit{How to transfer} the prior knowledge effectively should be carefully designed to prevent inefficient and negative transfer. Previous work \cite{davis2009deep,wang2014active,zhou2014multi} has used the generative probabilistic method. Bayesian learning methods can predict the target domain by combining the prior source distribution to generate a posterior distribution. Alternatively, max margin methods\cite{kuzborskij2013n,tommasi2010safety,yang2007cross} try to use the hyperplane parameter to transfer the knowledge between source and target domains. Luo et al. \cite{jie2011multiclass} proposed MKTL used feature augmentation method to leverage the source model.

Our work corresponds to the context above. In this paper, we propose EMTLe based on the model transfer approach. In our case, we focus on how to exploit the knowledge from the prediction of the source models instead of using the parameters of the model to transfer the knowledge.
