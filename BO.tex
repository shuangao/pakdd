In Eq. \eqref{eq:low_opt}, we have to find the optimal target function $f$ that minimize the training error on the target domain in addition with the source model $f'$ and the transfer parameter $\beta$. As we discussed before, the transfer parameter in Eq. \eqref{eq:low_opt} is a hyperparameter that is decided by the relatedness between the source model and target domain. A simple way to measure the relatedness is to evaluate the performance of the source model on the target training data. However, this estimate could lead to a relatively large variance when the target training data is small. In this paper, we use the leave-one-out cross-validation (LOOCV) strategy to reduce this variance. Previous research \cite{kuzborskij2013stability} suggests that LOOCV can increase the robustness of the estimated hyperparameter especially on small dataset. For the $i$-th round, the target set for training $D_T=\{x_i,y_i|i \in 1,...,l \}$ is split into training set $D_T^{\backslash i}=\{x_j,y_j|j\neq i\}$ and validation set $D_T^{i}=\{x_i,y_i\}$. The transfer parameter can be optimized with the following bi-level optimization (BO) function:
\begin{equation}\label{eq:BO}
\begin{aligned}
&\underset{\beta}{\arg \min}\sum_i^l\mathcal{L}(f^{i}(\beta),D_T^{i})\\
f^{i}(\beta)&=\underset{f \in \mathcal{F}}{\arg \min}\ell\left(f+\beta f',D_T^{\backslash i}\right) 
\end{aligned}
\end{equation} 
Here, we can use any convex objective function (e.g. SVM objective function) as $\ell(\cdot,\cdot)$ for the low-level optimization problem and $\mathcal{L}(\cdot,\cdot)$ is our the high-level cross-validation objective function.
In many previous works\cite{maclaurin2015gradient}\cite{Pedregosa16}, BO optimization is a non-convex problem and can only obtain the approximate solution. However, in our paper, we will show that problem \eqref{eq:BO} is strongly convex and we are able to obtain its optimal solution. 
\subsection{Low-level optimization problem using mean square loss}
To better illustrate our learning scenario, define our learning process as follows. Suppose we have $N$ visual categories and 
can obtain $N$ source binary classifiers $f'=\{f'_1,...,f'_N\}$ from the source domain. We want to train a target function $f$ consists of $N$ binary classifier $f=\{f_1,...,f_N\}$ using the target training set $D_T$ and the source models $f'$.
Specifically, in our BO problem Eq. \eqref{eq:BO}, for the low level optimization problem, we consider the scenario where each of the binary target model is a linear classifier $f_i = w_ix+b_i$.
Fand we use Least-square loss as the objective function to optimize each target model $f_n$ for any given transfer parameter $\beta$:
\begin{equation}\label{eq:bo_low}
\begin{aligned}
\text{Low-level:}\quad&f^{\backslash i}(\beta) : \min \sum_n^N\frac{1}{2}||w_n||^2+\frac{C}{2}\sum_j^l\left(Y_{jn}-f_n(x_j)-\beta_n f_n'(x_j)\right)^2\\
&\text{s.t.} \qquad x_j \in D_T^{\backslash i}
\end{aligned}
\end{equation}
Here, $Y$ is an encoded matrix of $y$ using one-hot strategy where $Y_{in} =1$ if $y_i=n$ and 0 otherwise.

The reason why we use the objective function \eqref{eq:bo_low} is that, it can provide an unbiased closed form Leave-one-out error estimation of each binary model $f_n$\cite{cawley2006leave}. 
Let $K(X,X)$ be the kernel matrix and
\begin{equation}\label{eq:linear}
\psi=\left[ 
{K(X,X) + \frac{1}{C}{\rm{I}}} \right]
\end{equation}
Let $\psi^{-1}$ is the inverse of matrix $\psi$ and  $\psi_{ii}^{-1}$ is the $ith$ diagonal element of $\psi^{-1}$. $\hat{Y}_{in}$, the LOO estimation of binary model $f_n$ for sample $x_i$, can be written as:
\begin{equation} \label{eq:loo}
{\hat Y_{in}} = {Y_{in}} - \frac{{{\alpha _{in}}}}{{\psi_{ii}^{ - 1}}}\quad {\text{for}}\quad n = 1,...,N
\end{equation}
where the matrix $\boldsymbol{\alpha}=\{\alpha_{in}|i=1,...l;n=1,...,N\}$ can be calculated as:
\begin{equation}
\boldsymbol{\alpha} =\psi^{-1} Y - \psi^{-1} f'(X)\boldsymbol{\beta}^T
\end{equation}

\subsection{High-level optimization problem using multi-class hinge loss with $\ell_2$ pentalty} 
For high level optimization problem, we use multi-class hinge loss\cite{crammer2002algorithmic} with $\ell_2$ penalty as our objective function.
\begin{equation}\label{eq:bo_high}
\begin{aligned}
\text{High-level:}\quad&\beta: \min \frac{{{\lambda}}}{2}\sum\limits_{n = 1}^N {{{\left\| {{\beta _n}} \right\|}^2}}  + \sum\limits_{i,n}\left[ {1 - {\varepsilon _{n{y_i}}} + {{\hat Y}_{in}} - {{\hat Y}_{i{y_i}}} - {\xi _i}} \right]\\
&\text{s.t.} \qquad1 - {\varepsilon _{n{y_i}}} + {\hat Y_{in}} - {\hat Y_{i{y_i}}} \le {\xi_i}
\end{aligned}
\end{equation}
Here, $\varepsilon _{n{y_i}}=1$ if $n=y_i$ otherwise 0.
Compared to the previous work \cite{tommasi2014learning}\cite{kuzborskij2013n} which uses the multi-class hinge loss without the $\ell_2$ penalty, there are two main advantages for our objective function: (1) When the training set is small, our LOOCV estimation could have a large variance. Similar to the penalty term in low-level problem \eqref{eq:bo_low},  the $\ell_2$ penalty here can {reduce this variance and improve the generalization ability of the estimated transfer parameter}. (2) With the $\ell_2$ penalty, optimization problem \eqref{eq:bo_high} become a strongly convex optimization problem w.r.t. the transfer parameter $\beta$. Therefore, we can obtain an $O({\log(t)}/{t})$ optimal solution with $t$ iterations using Algorithm \ref{alg:1} (see proof in Theorem \ref{th:1} in Appendix).
\input{agl1.tex} 