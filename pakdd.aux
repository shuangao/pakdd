\relax 
\citation{pan2010survey}
\citation{kuzborskij2013n,tommasi2014learning}
\citation{kuzborskij2013stability}
\citation{ben2010theory,ben2007analysis}
\citation{pan2010survey}
\citation{tommasi2014learning,fei2006one}
\citation{jie2011multiclass}
\@writefile{toc}{\contentsline {title}{Effective Multiclass Transfer For Hypothesis Transfer Learning}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{No Author Given}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{jie2011multiclass}
\citation{Pedregosa16}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}}
\newlabel{sec:work}{{2}{2}}
\citation{fei2006one}
\citation{davis2009deep}
\citation{wang2014active}
\citation{yang2007cross}
\citation{aytar2011tabula}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\citation{jie2011multiclass}
\citation{jie2011multiclass}
\@writefile{toc}{\contentsline {section}{\numberline {3}Using the Source Knowlege as the Auxiliary Bias}{3}}
\newlabel{sec:prob}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of feature augmentation in MKTL. $f_i'$ is the output of the $i$-th source model and $\beta _{in}$ is the hyperparameter (need to be estimated) to weigh the augmented feature. $\phi _n(x)$ is augmented feature for the $n$-th binary model.}}{3}}
\newlabel{fig:mktl}{{1}{3}}
\citation{aytar2011tabula,tommasi2014learning,yang2007adapting}
\citation{jie2011multiclass}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Demonstration of using the source class probability as the auxiliary bias to adjust the output of the target model.}}{4}}
\newlabel{fig:ab}{{2}{4}}
\newlabel{eq:low_opt}{{1}{4}}
\citation{Pedregosa16}
\citation{kuzborskij2013stability}
\citation{maclaurin2015gradient,Pedregosa16}
\@writefile{toc}{\contentsline {section}{\numberline {4}Bi-level Optimization for Transfer Parameter Estimation}{5}}
\newlabel{sec:smitle}{{4}{5}}
\newlabel{eq:BO}{{2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Low-level optimization problem}{5}}
\citation{cawley2006leave}
\citation{cawley2006leave}
\citation{crammer2002algorithmic}
\citation{kuzborskij2013n,tommasi2014learning}
\newlabel{eq:bo_low}{{3}{6}}
\newlabel{eq:linear}{{4}{6}}
\newlabel{eq:loo}{{5}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}High-level optimization problem}{6}}
\newlabel{eq:bo_high}{{7}{6}}
\citation{krizhevsky2012imagenet}
\citation{jie2011multiclass}
\citation{tommasi2014learning}
\newlabel{alg:1}{{4.2}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces EMTLe}}{7}}
\newlabel{alg:1}{{1}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{7}}
\newlabel{sec:exp}{{5}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Dataset \& Baseline methods}{7}}
\citation{jie2011multiclass,tommasi2014learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Statistics of the datasets and subsets}}{8}}
\newlabel{tab:class_info}{{1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Transfer from Single Source Domain}{8}}
\newlabel{a}{{3(a)}{9}}
\newlabel{sub@a}{{(a)}{9}}
\newlabel{b}{{3(b)}{9}}
\newlabel{sub@b}{{(b)}{9}}
\newlabel{c}{{3(c)}{9}}
\newlabel{sub@c}{{(c)}{9}}
\newlabel{d}{{3(d)}{9}}
\newlabel{sub@d}{{(d)}{9}}
\newlabel{e}{{3(e)}{9}}
\newlabel{sub@e}{{(e)}{9}}
\newlabel{f}{{3(f)}{9}}
\newlabel{sub@f}{{(f)}{9}}
\newlabel{g}{{3(g)}{9}}
\newlabel{sub@g}{{(g)}{9}}
\newlabel{h}{{3(h)}{9}}
\newlabel{sub@h}{{(h)}{9}}
\newlabel{i}{{3(i)}{9}}
\newlabel{sub@i}{{(i)}{9}}
\newlabel{j}{{3(j)}{9}}
\newlabel{sub@j}{{(j)}{9}}
\newlabel{k}{{3(k)}{9}}
\newlabel{sub@k}{{(k)}{9}}
\newlabel{l}{{3(l)}{9}}
\newlabel{sub@l}{{(l)}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Recognition accuracy for HTL domain adaptation from a single source. 5 different sizes of target training sets are used in each group of experiments. A, D, W and C denote the 4 subsets in Table 1\hbox {} respectively.}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {C$\rightarrow $A}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {D$\rightarrow $A}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {W$\rightarrow $A}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {A$\rightarrow $C}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {D$\rightarrow $C}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {W$\rightarrow $C}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {A$\rightarrow $D}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {C$\rightarrow $D}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {W$\rightarrow $D}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {A$\rightarrow $W}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {C$\rightarrow $W}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(l)}{\ignorespaces {D$\rightarrow $W}}}{9}}
\newlabel{fig:exp}{{3}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Transfer from Multiple Source Domains}{9}}
\citation{rockafellar2015convex}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The selected classes of the two source domains and the classifier type of the source model.}}{10}}
\newlabel{tab:class_gen}{{2}{10}}
\newlabel{a2}{{4(a)}{10}}
\newlabel{sub@a2}{{(a)}{10}}
\newlabel{b2}{{4(b)}{10}}
\newlabel{sub@b2}{{(b)}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Recognition Accuracy for Multi-Model \& Multi-Source experiment on two target datasets. }}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {D+W $\rightarrow $ A}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {D+W $\rightarrow $ C}}}{10}}
\newlabel{fig:exp2}{{4}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{10}}
\newlabel{th:1}{{1}{10}}
\bibstyle{splncs03}
\bibdata{research}
\bibcite{aytar2011tabula}{1}
\bibcite{ben2010theory}{2}
\bibcite{ben2007analysis}{3}
\newlabel{eq:app:strong}{{10}{11}}
\newlabel{eq:app:inner}{{11}{11}}
\newlabel{eq:app:squrediff}{{12}{11}}
\newlabel{eq:app:it_diff}{{13}{11}}
\newlabel{eq:app:difsum}{{14}{11}}
\newlabel{appd:convg}{{6}{11}}
\bibcite{cawley2006leave}{4}
\bibcite{crammer2002algorithmic}{5}
\bibcite{davis2009deep}{6}
\bibcite{fei2006one}{7}
\bibcite{jie2011multiclass}{8}
\bibcite{krizhevsky2012imagenet}{9}
\bibcite{kuzborskij2013stability}{10}
\bibcite{kuzborskij2013n}{11}
\bibcite{maclaurin2015gradient}{12}
\bibcite{pan2010survey}{13}
\bibcite{Pedregosa16}{14}
\bibcite{rockafellar2015convex}{15}
\bibcite{tommasi2014learning}{16}
\bibcite{wang2014active}{17}
\bibcite{yang2007adapting}{18}
\bibcite{yang2007cross}{19}
