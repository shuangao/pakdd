In this paper, we present a novel method called SMTLe that is able to transfer knowledge of the source model in domain adaptation. Inspired by previous work,
we propose a novel perspective on the work of HTL and show the reasons why positive and negative transfer would happen in the different scenarios. Based on our analysis, we propose our method SMTLe that can safely leverage the knowledge from the source models to achieve the improved target model performance by limiting the VC dimension of the transfer problem and reduce the empirical risk as well. Experiment results show that SMTLe can leverage related source knowledge and alleviate negative transfer in different scenarios and outperform other baseline methods.

In our perspective on the domain adaptation problem, the feature augmentation approach can fit a wider range of source classifiers. We can leverage the knowledge from any source model that can output the decision score/confidence, such as the Neural Networks and the inference model. 
Meanwhile, there are still many open issues to solve before we could maximize the utility of different kinds of source classifiers. For example, how to better exploit the knowledge from a deep neural network with our feature augmentation framework and achieve good positive transfer performance and avoid negative transfer simultaneously. These challenges could lead to our future interest.

