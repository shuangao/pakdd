Domain adaptation for image recognition tries to exploit the knowledge from a source domain with plentiful data to help learn a classifier for the target domain with a different distribution and little labeled training data. In domain adaptation, the source and target domains share the same label but their data are drawn from different distributions.

In domain adaptation, the knowledge of the source domain can be transferred by 3 different approaches: \textit{instance transfer}, \textit{model transfer} and \textit{feature representation transfer} \cite{pan2010survey}. In this paper, we focus on the method that transfers knowledge from the source model. Some recent works show that exploiting the knowledge from the source model can boost the performance of the target model effectively\cite{tommasi2014learning}\cite{kuzborskij2013n}.
Moreover, in some real applications, we can only obtain the source models and it is difficult to access their training data for different reasons such as the data credential.   
Recently, a framework called Hypothesis Transfer Learning (HTL) \cite{kuzborskij2013stability} has been proposed to handle this situation. HTL assumes only source models (called the \textit{hypotheses}) trained on the source domain can be utilized and there is no access to source data, nor any knowledge about the relatedness of the source and target distributions. 


Previous research \cite{ben2010theory} \cite{ben2007analysis} shows that without carefully measuring the distribution similarity between the source and target data, the source knowledge could not be exploited effectively or even hurt the learning process (called  \textit{negative transfer})\cite{pan2010survey}. 
However, as we are not able to access the source data in an HTL setting, how to effectively and safely exploit the knowledge from the source model could be an important issue in HTL (Safety issue). Moreover, different source models can be trained with different kinds of classifiers. For example most models trained from ImageNet are deep convolutional neural networks while some models of the VOC recognition task could be SVMs or ensemble models. Therefore, a practical HTL algorithm should be compatible with different types of source classifiers (Compatibility issue). To the best of our knowledge, none of the previous work in HTL is able to solve these two issues at the same time.

In this paper, we propose our method, {called Safe Multiclass Transfer Learning (SMTLe)}, that can solve these two issues simultaneously. Previous work \cite{jie2011multiclass} suggests that feature augmentation can greatly increase the compatibility of the target model in the HTL scenario. To solve the compatibility issue, we propose a feature augmentation method that for each target example, we add its class probabilities from the source model as the auxiliary feature. Moreover, we apply different weights (called transfer parameters) for different auxiliary features to control the amount of the knowledge transferred from each source model. As a result, the value of the transfer parameter reflects the similarity between the source and target domain. By carefully estimating the transfer parameters, we can obtain the optimal target model.

To better estimate the transfer parameters, we treat them as the hyperparameter of a convex optimization problem and  
introduce bi-level hyperparameter optimization\cite{Pedregosa16} , which has been widely used for many different hyperparameter optimization problem, to estimate the optimal values. Specifically, on the low-level optimization problems, we use a least-square SVM to obtain the hyperplane and on the high level, we use the novel multi-class hinge loss with $\ell_2$ penalty. Different from many other bi-level optimization problems which are non-convex optimization problems, we show that our transfer parameter estimation problem is a strongly convex optimization problem and demonstrate that our method SMTLe can find the $O({\log(t)}/{t})$ optimal solution with $t$ iteration. 

In our experiment, we use the popular benchmarks Office and Caltech256 as our dataset. We show that SMTLe can successfully transfer the knowledge with different types of source models. Moreover, we show that our novel high level objective function with $\ell_2$ penalty can improve the performance of the target model effectively compared with SMTLe without $\ell_2$ penalty and other baselines in HTL. 

The rest of this paper is organized as follows: In Section \ref{sec:work} we introduce the issues in transfer learning and some related work regarding these issues.
In Section \ref{sec:prob}, we reformulate the HTL in Phase I and propose our method of feature augmentation. We show that we can better analyze the performance of the transfer learning algorithm with feature augmentation. Then, we propose a novel objective function for transfer parameter estimation, called SMTLe in Section \ref{sec:smitle}. We show that the estimated transfer parameter can evaluate the utility of the source hypothesis and alleviate negative transfer autonomously. In Section \ref{sec:exp}, we show the performance comparison between SMTLe and other baselines on a variety of experiments on MNIST and USPS datasets.
